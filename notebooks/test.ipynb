{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "     table.dataframe {\n",
       "font-size:24px;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an initialization cell that is not part of the presentation.\n",
    "# This cell should be run while not in the rise mode\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0) # make notebook deterministic\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD # import model that we will use but do not want to emphasize\n",
    "\n",
    "# Disable warnings during the presentation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "\n",
    "#CSS customization\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "# set width cell to screen width\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# default font-size is 10pt\n",
    "# anyway the code below set font size for code cells\n",
    "HTML(\"\"\"<style>\n",
    ".CodeMirror pre {\n",
    "    font-size: 11pt;\n",
    "}\n",
    "</style>\"\"\")\n",
    "\n",
    "# increase font size of pd.DataFrame\n",
    "HTML(\"\"\"<style>\n",
    "     table.dataframe {\n",
    "font-size:24px;}\n",
    "</style>\"\"\")\n",
    "\n",
    "# to increase font size in a markdown cell\n",
    "#<font size=5px>test</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Pipelines and Gridsearch with scikit-learn\n",
    "\n",
    "#### May 16 2018\n",
    "\n",
    "## Florent Martin\n",
    "## Koen van Woerden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Goal of the talk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* <font size=8>**Pipeline**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* <font size=8>**Gridsearch**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* <font size=8>**Scikit-learn**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is a data pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../img/figures/diagram_pipeline.svg\" width =400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is a data pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr style=\"height:650px;width:900px;\">\n",
    "<td style=\"width:600px; height:700px;\"> <img src=\"../img/figures/diagram_pipeline.svg\" style=\"width:600px; height:700px;\"/></td>\n",
    "<td style=\"width:800px;\"> <img src=\"../img/figures/Trans-Alaska_Pipeline.jpg\" style=\"width:800px;\" />  </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Data | Liquid\n",
    "--- | ---  \n",
    "<img src=\"../img/figures/diagram_pipeline.svg\" width=1000> | <img src=\"../img/figures/Trans-Alaska_Pipeline.jpg\" width=100> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pipeline $\\Rightarrow$ easily experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../img/figures/diagram_two_pipelines.svg\" width = 1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Gridsearch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../img/figures/diagram_pipeline_hyperparamter.svg\" width = 1100></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gridsearch Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Goal**: find the best hyperparameter for logistic regression among  \n",
    "Regularization type: L1, L2  \n",
    "$C =0.1,1,10,100$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../img/figures/grid.svg\" width = 800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipeline + Gridsearch $\\Rightarrow$ scikit-learn to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../img/figures/diagram_pipeline_two_hyperparamters.svg\" width = 1100></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. **Classifying authors**  \n",
    "   $\\bullet$ Dataset  \n",
    "   $\\bullet$ Baseline model  \n",
    "  \n",
    "2. **Pipeline**  \n",
    "  $\\bullet$ Build your first pipeline  \n",
    "  $\\bullet$ Add new transformations   \n",
    "  $\\bullet$ Add non-scikit-learn transformations  \n",
    "  $\\bullet$ Keep experimenting  \n",
    "  \n",
    "3. **Gridsearch**  \n",
    "  $\\bullet$ Hyperparameters  \n",
    "  $\\bullet$ With scikit-learn transformations  \n",
    "  $\\bullet$ With non-scikit-learn transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Classifying authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table style='font-size:39px'>\n",
    "    <tr>\n",
    "        <td>INPUT</td> <td></td><td>OUTPUT</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>sentence</td>                                          <td>$\\Rightarrow$</td>  <td>author</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>'Even the very lights from the city bewilder him.' </td><td>$\\Rightarrow$</td>  <td>Edgar Allan Poe  </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>X</td>                                                 <td>$\\Rightarrow$</td>  <td>y</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/talk/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14684, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>\"Our first slide into the abyss itself, from t...</td>\n",
       "      <td>Poe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>He heard my account of the self dissolution of...</td>\n",
       "      <td>Shelley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5777</th>\n",
       "      <td>Nor has he yet had any difficulty in obtaining...</td>\n",
       "      <td>Lovecraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8462</th>\n",
       "      <td>We examined, first, the furniture of each apar...</td>\n",
       "      <td>Poe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>I did this at some little risk, and before clo...</td>\n",
       "      <td>Poe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     author\n",
       "10115  \"Our first slide into the abyss itself, from t...        Poe\n",
       "5906   He heard my account of the self dissolution of...    Shelley\n",
       "5777   Nor has he yet had any difficulty in obtaining...  Lovecraft\n",
       "8462   We examined, first, the furniture of each apar...        Poe\n",
       "1457   I did this at some little risk, and before clo...        Poe"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X, author = data['text'], data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poe          5963\n",
       "Shelley      4465\n",
       "Lovecraft    4256\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><font size=7>Poe</font></center> | <center><font size=7>Shelley</font></center> | <center><font size=7> Lovecraft </font></center>\n",
    "---|---|---\n",
    " <img src=\"../img/figures/poe.png\" width=265>| <img src=\"../img/figures/shelley.jpg\" width=300> | <img src=\"../img/figures/lovecraft.jpg\" width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# scikit-learn basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Objects have `fit` method\n",
    "* Objects have `transform` or `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Turn labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder.fit(author);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = label_encoder.transform(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shelley</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lovecraft</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shelley</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Poe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author  y\n",
       "0    Shelley  2\n",
       "1  Lovecraft  0\n",
       "2    Shelley  2\n",
       "3        Poe  1\n",
       "4        Poe  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'author': author[:5], 'y': y[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words: convert strings to vectors (one-hot encoding)\n",
    "<center><img src=\"../img/figures/Koen/03-kvw-bow.svg\" width=1350></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Baseline model: Bag of Words + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvec.fit(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_cvec = cvec.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"One\" said the clock.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_cvec.toarray()[87,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14684, 22476)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cvec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.fit(X_cvec, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### (Multi-class: One-versus-Rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Predict author of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rand_sentence = X.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I always attributed my failure at these points to the disordered state of his health.\n"
     ]
    }
   ],
   "source": [
    "print(rand_sentence.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rand_sentence_vec = cvec.transform(rand_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04726737, 0.85903555, 0.09369708]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.predict_proba(rand_sentence_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lovecraft', 'Poe', 'Shelley'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Poe'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = rand_sentence.index[0]\n",
    "author.loc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741214927812585"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.score(X_cvec, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generalization: try model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "val = pd.read_csv('../data/talk/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4895, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_val, author_val  = val['text'], val['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_val_cvec = cvec.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_val = label_encoder.transform(author_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175689479060265"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.score(X_val_cvec, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Repetitive code \n",
    "## Solution: pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  Combine all  transformations in a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "logistic_regression = LogisticRegression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_cvec = cvec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd = svd.fit_transform(X_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression.fit(X_svd, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4443612094797058"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.score(X_svd, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_val_cvec = cvec.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_svd = svd.transform(X_val_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44473953013278855"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression.score(X_val_svd, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "svd = TruncatedSVD()\n",
    "logistic_regression = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_cvec = cvec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_svd = svd.fit_transform(X_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression.fit(X_svd, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression.score(X_svd, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_val_cvec = cvec.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_val_svd = svd.transform(X_val_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression.score(X_val_svd, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* <font size=8>Many **intermediate variables** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <font size=8>Transformations spread out over the notebook</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <font size=8>**Experimenting** is **difficult**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <font size=8>**Solution**:  create a **Pipeline object**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=  [   ('cvec'  , CountVectorizer()), \n",
    "                                ('logreg', LogisticRegression())  ]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741214927812585"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175689479060265"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rand_sentence = X_val.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When, at length, we had concluded our examination, and the intense excitement of the time had, in some measure, subsided, Legrand, who saw that I was dying with impatience for a solution of this most extraordinary riddle, entered into a full detail of all the circumstances connected with it.\n"
     ]
    }
   ],
   "source": [
    "print(rand_sentence.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.63755557e-03, 9.90702703e-01, 6.59741126e-04]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict_proba(rand_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(rand_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lovecraft', 'Poe', 'Shelley'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2794    Lovecraft\n",
       "Name: author, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author[rand_sentence.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under the hood of Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=  [ ('first_transformation', first_transformation),\n",
    "                                ...\n",
    "                              ('last_transformation', last_transformation)          ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Scikit-learn does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_first = first_transformation.fit_transform(X)\n",
    "X_second = second_transformation.fit_transform(X_first)\n",
    "...\n",
    "X_last = last_transformation.fit(X_previous_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* All step but the last *must* implement a `fit` and `transform` method\n",
    "* The last step *must* implement a `fit` method, and  a `transform` or `predict` method as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Add a non-scikit-learn transformation to the Pipeline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**lemma** = **dictionary entry**  \n",
    "\n",
    "**swimming**, **swims**, **swim** $\\Rightarrow$ same **lemma** $\\Rightarrow$ **swim**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Lemmatizer**: word $\\mapsto$ lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##   nltk = natural language toolkit  (NLP library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Lemmatizer():\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        lem = WordNetLemmatizer()\n",
    "        lower = X.str.lower()\n",
    "        tokenized = lower.str.split(' ')\n",
    "        lemmatized = tokenized.apply(lambda l: \" \".join([lem.lemmatize(word) for word in l]))\n",
    "        return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence = pd.Series(data=['Cows and pigs are common animals on farms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cow and pig are common animal on farm'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.transform(sentence).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[  ('lem', Lemmatizer()),\n",
    "                             ('cvec', CountVectorizer()),\n",
    "                             ('logreg', LogisticRegression())   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720784527376737"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8145045965270684"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Adding Gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class GensimWord2Vec():\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = Word2Vec(X)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        lower = X.str.lower()\n",
    "        tokenized = lower.str.split(' ')\n",
    "        vectors = tokenized.apply(lambda l: [self.model[word] for word in l if word in self.model])\n",
    "        def average(l):\n",
    "            if l == []:\n",
    "                return np.zeros(self.model.vector_size)\n",
    "            else:\n",
    "                return np.mean(l, axis=0)\n",
    "        vectors = vectors.apply(average)\n",
    "        vectors = vectors.apply(pd.Series)\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=  [   ('word2vec', GensimWord2Vec()),\n",
    "                                ('logreg', LogisticRegression())    ]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4167120675565241"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4067415730337079"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Feature unions: Combine bag of words and word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../img/figures/diagram_feature_union1.svg\" width=1100></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lem_cvec = Pipeline(steps = [('lem', Lemmatizer()),\n",
    "                             ('cvec', CountVectorizer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature_union = FeatureUnion([('lem_cvec', lem_cvec),\n",
    "                              ('gensimw2v', GensimWord2Vec())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline( [  ('feature_union', feature_union),\n",
    "                        ('logreg', LogisticRegression())                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733042767638246"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8177732379979571"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Further experiment: tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=  [  ('lem', Lemmatizer()),\n",
    "                               ('tfidf', TfidfVectorizer()),\n",
    "                               ('logreg', LogisticRegression())    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8962816671206756"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802655771195097"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Further experiment: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=  [    ('CountVectorizer', CountVectorizer()),\n",
    "                                 ('NaiveBayes', MultinomialNB())             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156224461999455"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8330949948927477"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 3: Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a **hyperparameter**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Examples\n",
    "* learning rate \n",
    "* regularization coefficient\n",
    "* number of hidden layers in a neural network\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Responsibility of the data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font size=7>change **hyperparameter** $\\Rightarrow$ change **model** $\\Rightarrow$ change **performance**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_cvec = cvec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `C` is a **hyperparameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How do we know about `C`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(C=1000)\n",
    "logistic_regression.fit(X_cvec, y);\n",
    "logistic_regression.score(X_cvec, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7691521961184883"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_cvec = cvec.transform(X_val)\n",
    "logistic_regression.score(X_val_cvec, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Our previous model depends on a **hyperparameter** `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Changing `C` changes the performance $\\Rightarrow$  **Try** different `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Keep track of the results! (Who remembers the results we got?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We want this to be done automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gridsearch is what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gridsearch in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(estimator=LogisticRegression(), \n",
    "                  param_grid={'C': [0.1, 1, 10, 100, 1000]}, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] .................. C=0.1, score=0.7757352941176471, total=   0.4s\n",
      "[CV] C=0.1 ...........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=0.1, score=0.7662921348314606, total=   0.4s\n",
      "[CV] C=0.1 ...........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=0.1, score=0.774167177600654, total=   0.4s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .................... C=1, score=0.8112745098039216, total=   0.8s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .................... C=1, score=0.8032686414708886, total=   0.7s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .................... C=1, score=0.8078888207643572, total=   0.6s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................... C=10, score=0.7998366013071896, total=   1.3s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ..................... C=10, score=0.79244126659857, total=   1.2s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................... C=10, score=0.7968526466380543, total=   1.1s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] .................. C=100, score=0.7792075163398693, total=   1.4s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] ................... C=100, score=0.780388151174668, total=   1.5s\n",
      "[CV] C=100 ...........................................................\n",
      "[CV] .................. C=100, score=0.7819333742080523, total=   1.3s\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] .................. C=1000, score=0.769812091503268, total=   1.7s\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] ................. C=1000, score=0.7671092951991828, total=   1.4s\n",
      "[CV] C=1000 ..........................................................\n",
      "[CV] .................. C=1000, score=0.768444716942571, total=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10, 100, 1000]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=3)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(X_cvec, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074775265595205"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.40538804, 0.68529201, 1.20883743, 1.40308134, 1.50358335]),\n",
       " 'mean_score_time': array([0.00154662, 0.00148304, 0.0015765 , 0.0012993 , 0.0013814 ]),\n",
       " 'mean_test_score': array([0.77206483, 0.80747753, 0.79637701, 0.7805094 , 0.76845546]),\n",
       " 'mean_train_score': array([0.87806463, 0.97950149, 0.99819533, 0.99979571, 1.        ]),\n",
       " 'param_C': masked_array(data=[0.1, 1, 10, 100, 1000],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1}, {'C': 1}, {'C': 10}, {'C': 100}, {'C': 1000}],\n",
       " 'rank_test_score': array([4, 1, 2, 3, 5], dtype=int32),\n",
       " 'split0_test_score': array([0.77573529, 0.81127451, 0.7998366 , 0.77920752, 0.76981209]),\n",
       " 'split0_train_score': array([0.87832039, 0.97987331, 0.99856968, 0.99989783, 1.        ]),\n",
       " 'split1_test_score': array([0.76629213, 0.80326864, 0.79244127, 0.78038815, 0.7671093 ]),\n",
       " 'split1_train_score': array([0.87874144, 0.97875166, 0.99785473, 0.99979569, 1.        ]),\n",
       " 'split2_test_score': array([0.77416718, 0.80788882, 0.79685265, 0.78193337, 0.76844472]),\n",
       " 'split2_train_score': array([0.87713206, 0.97987948, 0.99816158, 0.9996936 , 1.        ]),\n",
       " 'std_fit_time': array([0.01688298, 0.06288269, 0.06525709, 0.05910731, 0.14008487]),\n",
       " 'std_score_time': array([1.41509946e-04, 1.21217710e-04, 1.33423879e-04, 4.64491501e-06,\n",
       "        1.21439263e-04]),\n",
       " 'std_test_score': array([0.00413201, 0.00328157, 0.00303805, 0.00111611, 0.00110353]),\n",
       " 'std_train_score': array([6.81463842e-04, 5.30212406e-04, 2.92848128e-04, 8.33797837e-05,\n",
       "        0.00000000e+00])}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation (CV): no need for separate validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='../img/figures/K-fold_cross_validation_EN.jpg' width=750>\n",
    "\n",
    "<font size=3>By Fabian Flöck [<a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>], <a href=\"https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.jpg\">from Wikimedia Commons</a></font>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(estimator=LogisticRegression(), \n",
    "                  param_grid={'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l1, score=0.6701388888888888, total=   0.2s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... C=0.1, penalty=l1, score=0.6649642492339122, total=   0.2s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l1, score=0.6740241160842019, total=   0.2s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.7757352941176471, total=   0.5s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...... C=0.1, penalty=l2, score=0.7662921348314606, total=   0.4s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ....... C=0.1, penalty=l2, score=0.774167177600654, total=   0.4s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.7794117647058824, total=   0.2s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.7758937691521961, total=   0.2s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........ C=1, penalty=l1, score=0.7792765174739423, total=   0.2s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.8112745098039216, total=   0.7s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.8032686414708886, total=   0.7s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........ C=1, penalty=l2, score=0.8078888207643572, total=   0.6s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.7847222222222222, total=   0.5s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.7709908069458631, total=   0.5s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....... C=10, penalty=l1, score=0.7780502759043532, total=   0.5s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.7998366013071896, total=   1.3s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ......... C=10, penalty=l2, score=0.79244126659857, total=   1.2s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....... C=10, penalty=l2, score=0.7968526466380543, total=   1.2s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...... C=100, penalty=l1, score=0.7730800653594772, total=   0.6s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...... C=100, penalty=l1, score=0.7605720122574056, total=   0.6s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...... C=100, penalty=l1, score=0.7602697731453096, total=   0.6s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...... C=100, penalty=l2, score=0.7792075163398693, total=   1.2s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ....... C=100, penalty=l2, score=0.780388151174668, total=   1.4s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...... C=100, penalty=l2, score=0.7819333742080523, total=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   15.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(X_cvec, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074775265595205"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.16640369, 0.39961926, 0.20294325, 0.64799929, 0.47372135,\n",
       "        1.210121  , 0.58727264, 1.25881402]),\n",
       " 'mean_score_time': array([0.00152787, 0.00150196, 0.00168991, 0.00143298, 0.00150402,\n",
       "        0.00163635, 0.00164938, 0.00144323]),\n",
       " 'mean_test_score': array([0.66970853, 0.77206483, 0.77819395, 0.80747753, 0.77792155,\n",
       "        0.79637701, 0.76464179, 0.7805094 ]),\n",
       " 'mean_train_score': array([0.69415719, 0.87806463, 0.92587179, 0.97950149, 0.99894445,\n",
       "        0.99819533, 1.        , 0.99979571]),\n",
       " 'param_C': masked_array(data=[0.1, 0.1, 1, 1, 10, 10, 100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2'],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1, 'penalty': 'l1'},\n",
       "  {'C': 0.1, 'penalty': 'l2'},\n",
       "  {'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'},\n",
       "  {'C': 100, 'penalty': 'l1'},\n",
       "  {'C': 100, 'penalty': 'l2'}],\n",
       " 'rank_test_score': array([8, 6, 4, 1, 5, 2, 7, 3], dtype=int32),\n",
       " 'split0_test_score': array([0.67013889, 0.77573529, 0.77941176, 0.81127451, 0.78472222,\n",
       "        0.7998366 , 0.77308007, 0.77920752]),\n",
       " 'split0_train_score': array([0.69646506, 0.87832039, 0.92541888, 0.97987331, 0.99918267,\n",
       "        0.99856968, 1.        , 0.99989783]),\n",
       " 'split1_test_score': array([0.66496425, 0.76629213, 0.77589377, 0.80326864, 0.77099081,\n",
       "        0.79244127, 0.76057201, 0.78038815]),\n",
       " 'split1_train_score': array([0.69486158, 0.87874144, 0.92798039, 0.97875166, 0.99887629,\n",
       "        0.99785473, 1.        , 0.99979569]),\n",
       " 'split2_test_score': array([0.67402412, 0.77416718, 0.77927652, 0.80788882, 0.77805028,\n",
       "        0.79685265, 0.76026977, 0.78193337]),\n",
       " 'split2_train_score': array([0.69114493, 0.87713206, 0.92421612, 0.97987948, 0.99877438,\n",
       "        0.99816158, 1.        , 0.9996936 ]),\n",
       " 'std_fit_time': array([0.0160719 , 0.04211747, 0.00613337, 0.05022431, 0.0177495 ,\n",
       "        0.03817193, 0.03865419, 0.12212861]),\n",
       " 'std_score_time': array([1.33795858e-04, 4.18686981e-05, 2.90025005e-04, 8.08133091e-05,\n",
       "        2.16157296e-04, 4.82616148e-04, 1.09047421e-04, 1.10812471e-04]),\n",
       " 'std_test_score': array([0.00371093, 0.00413201, 0.0016275 , 0.00328157, 0.00560704,\n",
       "        0.00303805, 0.00596926, 0.00111611]),\n",
       " 'std_train_score': array([2.22831334e-03, 6.81463842e-04, 1.56977265e-03, 5.30212406e-04,\n",
       "        1.73510891e-04, 2.92848128e-04, 0.00000000e+00, 8.33797837e-05])}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How to optimize hyperparameters of pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This works automatically with pipelines of scikit-learn objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gridsearch on pipelines of scikit-learn objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=  [   ('CountVectorizer', CountVectorizer()),\n",
    "                                ('NaiveBayes', MultinomialNB())           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CountVectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'CountVectorizer__analyzer': 'word',\n",
       " 'CountVectorizer__binary': False,\n",
       " 'CountVectorizer__decode_error': 'strict',\n",
       " 'CountVectorizer__dtype': numpy.int64,\n",
       " 'CountVectorizer__encoding': 'utf-8',\n",
       " 'CountVectorizer__input': 'content',\n",
       " 'CountVectorizer__lowercase': True,\n",
       " 'CountVectorizer__max_df': 1.0,\n",
       " 'CountVectorizer__max_features': None,\n",
       " 'CountVectorizer__min_df': 1,\n",
       " 'CountVectorizer__ngram_range': (1, 1),\n",
       " 'CountVectorizer__preprocessor': None,\n",
       " 'CountVectorizer__stop_words': None,\n",
       " 'CountVectorizer__strip_accents': None,\n",
       " 'CountVectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'CountVectorizer__tokenizer': None,\n",
       " 'CountVectorizer__vocabulary': None,\n",
       " 'NaiveBayes': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'NaiveBayes__alpha': 1.0,\n",
       " 'NaiveBayes__class_prior': None,\n",
       " 'NaiveBayes__fit_prior': True,\n",
       " 'memory': None,\n",
       " 'steps': [('CountVectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('NaiveBayes', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "MultinomialNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {  'CountVectorizer__binary': [True, False],\n",
    "                'CountVectorizer__ngram_range': [(1, 1), (1,2)],\n",
    "                'NaiveBayes__alpha': np.logspace(start=-1, stop=1, num=3)    }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(estimator=pipeline, param_grid=param_grid, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1, score=0.835171568627451, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1, score=0.8312563840653728, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1, score=0.836092376864909, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0, score=0.8339460784313726, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0, score=0.8241062308478039, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0, score=0.8346617617003883, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0, score=0.735498366013072, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0, score=0.726046986721144, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0, score=0.7124463519313304, total=   0.4s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1, score=0.8423202614379085, total=   1.2s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1, score=0.8330949948927477, total=   1.2s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1, score=0.8438585734723074, total=   1.1s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0, score=0.8331290849673203, total=   1.1s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0, score=0.8286006128702758, total=   1.2s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0, score=0.8256693235234007, total=   1.2s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0, score=0.7105800653594772, total=   1.1s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0, score=0.7023493360572012, total=   1.2s\n",
      "[CV] CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=True, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0, score=0.6901696300837932, total=   1.2s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1, score=0.8372140522875817, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1, score=0.8308478038815117, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=0.1, score=0.8373186184344983, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0, score=0.8327205882352942, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0, score=0.8257405515832482, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=1.0, score=0.8285305538524422, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0, score=0.7191584967320261, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0, score=0.7074565883554648, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 1), NaiveBayes__alpha=10.0, score=0.6960964643368077, total=   0.4s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1, score=0.8431372549019608, total=   1.2s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1, score=0.8330949948927477, total=   1.2s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=0.1, score=0.8403842223584713, total=   1.3s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0, score=0.8298611111111112, total=   1.3s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0, score=0.822267620020429, total=   1.2s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=1.0, score=0.8230124667892909, total=   1.2s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0, score=0.6785130718954249, total=   1.2s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0, score=0.669458631256384, total=   1.2s\n",
      "[CV] CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0 \n",
      "[CV]  CountVectorizer__binary=False, CountVectorizer__ngram_range=(1, 2), NaiveBayes__alpha=10.0, score=0.6615573267933783, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   42.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('CountVectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), p...one, vocabulary=None)), ('NaiveBayes', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'CountVectorizer__binary': [True, False], 'CountVectorizer__ngram_range': [(1, 1), (1, 2)], 'NaiveBayes__alpha': array([ 0.1,  1. , 10. ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8397575592481613"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CountVectorizer__binary': True,\n",
       " 'CountVectorizer__ngram_range': (1, 2),\n",
       " 'NaiveBayes__alpha': 0.1}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.27160931, 0.28333004, 0.26002137, 0.93489639, 0.92432928,\n",
       "        0.91905975, 0.25934068, 0.27192235, 0.26511423, 0.95985969,\n",
       "        0.98999206, 0.95550092]),\n",
       " 'mean_score_time': array([0.13309948, 0.13329951, 0.11747607, 0.2447079 , 0.25512131,\n",
       "        0.24656749, 0.11637505, 0.11698055, 0.11752685, 0.27359398,\n",
       "        0.2369833 , 0.26216181]),\n",
       " 'mean_test_score': array([0.83417325, 0.83090439, 0.7246663 , 0.83975756, 0.82913375,\n",
       "        0.70103514, 0.83512667, 0.82899755, 0.70757287, 0.83887224,\n",
       "        0.82504767, 0.66984473]),\n",
       " 'mean_train_score': array([0.95215891, 0.92764245, 0.79807968, 0.99785482, 0.9899891 ,\n",
       "        0.86764519, 0.94834534, 0.92178569, 0.77397189, 0.99775269,\n",
       "        0.98842278, 0.83369682]),\n",
       " 'param_CountVectorizer__binary': masked_array(data=[True, True, True, True, True, True, False, False,\n",
       "                    False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_CountVectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 2), (1, 2), (1, 2), (1, 1),\n",
       "                    (1, 1), (1, 1), (1, 2), (1, 2), (1, 2)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_NaiveBayes__alpha': masked_array(data=[0.1, 1.0, 10.0, 0.1, 1.0, 10.0, 0.1, 1.0, 10.0, 0.1,\n",
       "                    1.0, 10.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'CountVectorizer__binary': True,\n",
       "   'CountVectorizer__ngram_range': (1, 1),\n",
       "   'NaiveBayes__alpha': 0.1},\n",
       "  {'CountVectorizer__binary': True,\n",
       "   'CountVectorizer__ngram_range': (1, 1),\n",
       "   'NaiveBayes__alpha': 1.0},\n",
       "  {'CountVectorizer__binary': True,\n",
       "   'CountVectorizer__ngram_range': (1, 1),\n",
       "   'NaiveBayes__alpha': 10.0},\n",
       "  {'CountVectorizer__binary': True,\n",
       "   'CountVectorizer__ngram_range': (1, 2),\n",
       "   'NaiveBayes__alpha': 0.1},\n",
       "  {'CountVectorizer__binary': True,\n",
       "   'CountVectorizer__ngram_range': (1, 2),\n",
       "   'NaiveBayes__alpha': 1.0},\n",
       "  {'CountVectorizer__binary': True,\n",
       "   'CountVectorizer__ngram_range': (1, 2),\n",
       "   'NaiveBayes__alpha': 10.0},\n",
       "  {'CountVectorizer__binary': False,\n",
       "   'CountVectorizer__ngram_range': (1, 1),\n",
       "   'NaiveBayes__alpha': 0.1},\n",
       "  {'CountVectorizer__binary': False,\n",
       "   'CountVectorizer__ngram_range': (1, 1),\n",
       "   'NaiveBayes__alpha': 1.0},\n",
       "  {'CountVectorizer__binary': False,\n",
       "   'CountVectorizer__ngram_range': (1, 1),\n",
       "   'NaiveBayes__alpha': 10.0},\n",
       "  {'CountVectorizer__binary': False,\n",
       "   'CountVectorizer__ngram_range': (1, 2),\n",
       "   'NaiveBayes__alpha': 0.1},\n",
       "  {'CountVectorizer__binary': False,\n",
       "   'CountVectorizer__ngram_range': (1, 2),\n",
       "   'NaiveBayes__alpha': 1.0},\n",
       "  {'CountVectorizer__binary': False,\n",
       "   'CountVectorizer__ngram_range': (1, 2),\n",
       "   'NaiveBayes__alpha': 10.0}],\n",
       " 'rank_test_score': array([ 4,  5,  9,  1,  6, 11,  3,  7, 10,  2,  8, 12], dtype=int32),\n",
       " 'split0_test_score': array([0.83517157, 0.83394608, 0.73549837, 0.84232026, 0.83312908,\n",
       "        0.71058007, 0.83721405, 0.83272059, 0.7191585 , 0.84313725,\n",
       "        0.82986111, 0.67851307]),\n",
       " 'split0_train_score': array([0.95422967, 0.92991418, 0.80138946, 0.99785452, 0.9903964 ,\n",
       "        0.86881896, 0.95167552, 0.92439722, 0.7772783 , 0.99785452,\n",
       "        0.98876175, 0.83673886]),\n",
       " 'split1_test_score': array([0.83125638, 0.82410623, 0.72604699, 0.83309499, 0.82860061,\n",
       "        0.70234934, 0.8308478 , 0.82574055, 0.70745659, 0.83309499,\n",
       "        0.82226762, 0.66945863]),\n",
       " 'split1_train_score': array([0.95055675, 0.92603943, 0.79517826, 0.99805905, 0.98927367,\n",
       "        0.86791296, 0.94626622, 0.9189907 , 0.77229543, 0.99805905,\n",
       "        0.98804781, 0.83328226]),\n",
       " 'split2_test_score': array([0.83609238, 0.83466176, 0.71244635, 0.84385857, 0.82566932,\n",
       "        0.69016963, 0.83731862, 0.82853055, 0.69609646, 0.84038422,\n",
       "        0.82301247, 0.66155733]),\n",
       " 'split2_train_score': array([0.95169033, 0.92697375, 0.79767133, 0.9976509 , 0.99029721,\n",
       "        0.86620366, 0.94709427, 0.92196916, 0.77234195, 0.9973445 ,\n",
       "        0.98845879, 0.83106935]),\n",
       " 'std_fit_time': array([0.00403185, 0.00177807, 0.0068331 , 0.02494809, 0.02861644,\n",
       "        0.01859041, 0.004587  , 0.01306718, 0.00863473, 0.02028726,\n",
       "        0.06883398, 0.02735654]),\n",
       " 'std_score_time': array([0.00487018, 0.00271713, 0.00750015, 0.01781103, 0.01624278,\n",
       "        0.01274468, 0.00318593, 0.00209095, 0.00364173, 0.01398805,\n",
       "        0.00543052, 0.01167577]),\n",
       " 'std_test_score': array([0.00209662, 0.00481614, 0.0094613 , 0.00475306, 0.00306863,\n",
       "        0.00838405, 0.00302607, 0.00286889, 0.00941523, 0.00423712,\n",
       "        0.00341786, 0.00692742]),\n",
       " 'std_train_score': array([0.00153564, 0.00165102, 0.0025521 , 0.00016662, 0.0005075 ,\n",
       "        0.00108435, 0.00237894, 0.00221101, 0.00233806, 0.00030047,\n",
       "        0.00029257, 0.00233306])}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.set_params(**gridsearch.best_params_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9969354399346227"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8473953013278857"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adding non-sklearn objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gridsearch $\\Rightarrow$ derive from `BaseEstimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`fit_transform` $\\Rightarrow$ derive from `TransformerMixin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class GensimWord2Vec(TransformerMixin, BaseEstimator): # Derive from BaseEstimator!\n",
    "    def __init__(self, size=100, min_count=5):\n",
    "        self.size=size\n",
    "        self.min_count=min_count\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model = Word2Vec(X, size=self.size, min_count=self.min_count)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        lower = X.str.lower()\n",
    "        tokenized = lower.str.split(' ')\n",
    "        vectors = tokenized.apply(lambda l: [self.model[word] for word in l if word in self.model])\n",
    "        def average(l):\n",
    "            if l == []:\n",
    "                return np.zeros(self.model.vector_size)\n",
    "            else:\n",
    "                return np.mean(l, axis=0)\n",
    "        vectors = vectors.apply(average)\n",
    "        vectors = vectors.apply(pd.Series)\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=  [  ('word2vec', GensimWord2Vec()),\n",
    "                               ('logreg', LogisticRegression())        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'logreg__C': 1.0,\n",
       " 'logreg__class_weight': None,\n",
       " 'logreg__dual': False,\n",
       " 'logreg__fit_intercept': True,\n",
       " 'logreg__intercept_scaling': 1,\n",
       " 'logreg__max_iter': 100,\n",
       " 'logreg__multi_class': 'ovr',\n",
       " 'logreg__n_jobs': 1,\n",
       " 'logreg__penalty': 'l2',\n",
       " 'logreg__random_state': None,\n",
       " 'logreg__solver': 'liblinear',\n",
       " 'logreg__tol': 0.0001,\n",
       " 'logreg__verbose': 0,\n",
       " 'logreg__warm_start': False,\n",
       " 'memory': None,\n",
       " 'steps': [('word2vec', GensimWord2Vec(min_count=5, size=100)),\n",
       "  ('logreg',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False))],\n",
       " 'word2vec': GensimWord2Vec(min_count=5, size=100),\n",
       " 'word2vec__min_count': 5,\n",
       " 'word2vec__size': 100}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {   'word2vec__min_count': [1],\n",
    "                 'word2vec__size': [10, 50]           } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(estimator=pipeline, param_grid=param_grid, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] word2vec__min_count=1, word2vec__size=10 ........................\n",
      "[CV]  word2vec__min_count=1, word2vec__size=10, score=0.4097222222222222, total=   4.8s\n",
      "[CV] word2vec__min_count=1, word2vec__size=10 ........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  word2vec__min_count=1, word2vec__size=10, score=0.4218590398365679, total=   4.6s\n",
      "[CV] word2vec__min_count=1, word2vec__size=10 ........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   13.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  word2vec__min_count=1, word2vec__size=10, score=0.41630901287553645, total=   4.7s\n",
      "[CV] word2vec__min_count=1, word2vec__size=50 ........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   20.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  word2vec__min_count=1, word2vec__size=50, score=0.41013071895424835, total=   5.2s\n",
      "[CV] word2vec__min_count=1, word2vec__size=50 ........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   27.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  word2vec__min_count=1, word2vec__size=50, score=0.4216547497446374, total=   5.0s\n",
      "[CV] word2vec__min_count=1, word2vec__size=50 ........................\n",
      "[CV]  word2vec__min_count=1, word2vec__size=50, score=0.41651338647046804, total=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   42.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('word2vec', GensimWord2Vec(min_count=5, size=100)), ('logreg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'word2vec__min_count': [1], 'word2vec__size': [10, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=5)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word2vec__min_count': 1, 'word2vec__size': 50}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4160991555434487"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.set_params(**gridsearch.best_params_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4167120675565241"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4071501532175689"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <font style=\"font-size:60px;\"> **Pipelines** $\\Rightarrow$  **clear code** + **easy experiments** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  <font style=\"font-size:60px;\"> **Gridsearch** $\\Rightarrow$ **tuning** of **hyperparameters** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* <font style=\"font-size:60px;\">**Scikit-learn** $\\Rightarrow$ convenient classes for both</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you for your attention"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
